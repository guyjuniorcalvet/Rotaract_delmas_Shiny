View(Wage)
set.seed(1)
mse <- rep(NA, 10)
for (d in 1:10) {
fit <- train(wage ~ poly(Wage$age, d), data = Wage$wage, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
set.seed(1)
mse <- rep(NA, 10)
for (d in 1:10) {
poly_terms <- poly(Wage$age, d)
df <- cbind(wage = Wage$wage, as.data.frame(poly_terms))
fit <- train(wage ~ ., data = df, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
best_degree <- which.min(mse)
plot(1:10, mse, type = "b", xlab = "Degré", ylab = "MSE")
points(best_degree, mse[best_degree], col = "red", pch = 19)
# Ajustement
fit_final <- lm(wage ~ poly(age, best_degree), data = Wage)
ggplot(Wage, aes(age, wage)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", formula = y ~ poly(x, best_degree))
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(ISLR)
library(leaps)
library(splines)
library(gam)
library(caret)
f1 <- function(x, beta, xi) {
beta[1] + beta[2]*x + beta[3]*x^2 + beta[4]*x^3
}
f2 <- function(x, beta, xi) {
beta[1] + beta[2]*x + beta[3]*x^2 + beta[4]*x^3 + beta[5]*(x - xi)^3
}
# Continuité
d_continuity <- function(beta, xi) {
f1(xi, beta, xi) == f2(xi, beta, xi)
}
# Dérivée première
d1 <- function(x, beta, xi) {
beta[2] + 2*beta[3]*x + 3*beta[4]*x^2
}
d2 <- function(x, beta, xi) {
beta[2] + 2*beta[3]*x + 3*beta[4]*x^2 + 3*beta[5]*(x - xi)^2
}
# Dérivée seconde
d1_prime <- function(x, beta, xi) {
2*beta[3] + 6*beta[4]*x
}
d2_prime <- function(x, beta, xi) {
2*beta[3] + 6*beta[4]*x + 6*beta[5]*(x - xi)
}
plot(NA, xlim = c(0, 1), ylim = c(0, 1), xlab = "x", ylab = "g chapeau")
text(0.5, 0.9, "m=0 → constante")
text(0.5, 0.75, "m=1 → droite")
text(0.5, 0.6, "m=2 → polynôme degré 1")
text(0.5, 0.45, "m=3 → polynôme degré 2")
text(0.5, 0.3, "λ=0 → surajustement total")
X <- seq(-2, 2, length.out = 100)
Y <- 1 + 1*X + (-2)*(X - 1)^2 * (X >= 1)
df <- data.frame(X, Y)
ggplot(df, aes(X, Y)) + geom_line() + theme_minimal()
X <- seq(-2, 6, length.out = 100)
b1 <- (X >= 0 & X <= 2) - (X - 1)*(X >= 1 & X <= 2)
b2 <- (X - 3)*(X >= 3 & X <= 4) + (X > 4 & X <= 5)
Y <- 1 + 1*b1 + 3*b2
df <- data.frame(X, Y)
ggplot(df, aes(X, Y)) + geom_line() + theme_minimal()
wage <- data(Wage)
head(Wage)
set.seed(1)
mse <- rep(NA, 10)
for (d in 1:10) {
poly_terms <- poly(Wage$age, d)
df <- cbind(wage = Wage$wage, as.data.frame(poly_terms))
fit <- train(wage ~ ., data = df, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
best_degree <- which.min(mse)
plot(1:10, mse, type = "b", xlab = "Degré", ylab = "MSE")
points(best_degree, mse[best_degree], col = "red", pch = 19)
# Ajustement
fit_final <- lm(wage ~ poly(age, best_degree), data = Wage)
ggplot(Wage, aes(age, wage)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", formula = y ~ poly(x, best_degree))
cv_err <- rep(NA, 10)
for (k in 2:10) {
Wage$age_cut <- cut(Wage$age, k)
fit <- train(wage ~ age_cut, data = Wage, method = "lm", trControl = trainControl(method = "cv", number = 10))
cv_err[k] <- mean(fit$resample$RMSE^2)
}
best_k <- which.min(cv_err)
Wage$age_cut <- cut(Wage$age, best_k)
fit_step <- lm(wage ~ age_cut, data = Wage)
plot(Wage$age, Wage$wage, pch = 19, cex = 0.5)
lines(Wage$age, predict(fit_step), col = "blue")
gam_fit <- gam(wage ~ s(age, 4) + maritl + jobclass, data = Wage)
plot(gam_fit, se = TRUE, col = "blue")
summary(gam_fit)
data(Auto)
plot(Auto$horsepower, Auto$mpg)
model_poly <- lm(mpg ~ poly(horsepower, 3), data = Auto)
lines(Auto$horsepower, predict(model_poly), col = "red")
summary(model_poly)
data(Boston)
# Cubic polynomial
poly_fit <- lm(nox ~ poly(dis, 3), data = Boston)
Boston <- data(Boston)
head(Boston)
boston <- data(Boston)
data(Boston)
head(Boston)
head(grivr)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(ISLR)
library(leaps)
library(splines)
library(gam)
library(caret)
f1 <- function(x, beta, xi) {
beta[1] + beta[2]*x + beta[3]*x^2 + beta[4]*x^3
}
f2 <- function(x, beta, xi) {
beta[1] + beta[2]*x + beta[3]*x^2 + beta[4]*x^3 + beta[5]*(x - xi)^3
}
# Continuité
d_continuity <- function(beta, xi) {
f1(xi, beta, xi) == f2(xi, beta, xi)
}
# Dérivée première
d1 <- function(x, beta, xi) {
beta[2] + 2*beta[3]*x + 3*beta[4]*x^2
}
d2 <- function(x, beta, xi) {
beta[2] + 2*beta[3]*x + 3*beta[4]*x^2 + 3*beta[5]*(x - xi)^2
}
# Dérivée seconde
d1_prime <- function(x, beta, xi) {
2*beta[3] + 6*beta[4]*x
}
d2_prime <- function(x, beta, xi) {
2*beta[3] + 6*beta[4]*x + 6*beta[5]*(x - xi)
}
plot(NA, xlim = c(0, 1), ylim = c(0, 1), xlab = "x", ylab = "g chapeau")
text(0.5, 0.9, "m=0 → constante")
text(0.5, 0.75, "m=1 → droite")
text(0.5, 0.6, "m=2 → polynôme degré 1")
text(0.5, 0.45, "m=3 → polynôme degré 2")
text(0.5, 0.3, "λ=0 → surajustement total")
X <- seq(-2, 2, length.out = 100)
Y <- 1 + 1*X + (-2)*(X - 1)^2 * (X >= 1)
df <- data.frame(X, Y)
ggplot(df, aes(X, Y)) + geom_line() + theme_minimal()
X <- seq(-2, 6, length.out = 100)
b1 <- (X >= 0 & X <= 2) - (X - 1)*(X >= 1 & X <= 2)
b2 <- (X - 3)*(X >= 3 & X <= 4) + (X > 4 & X <= 5)
Y <- 1 + 1*b1 + 3*b2
df <- data.frame(X, Y)
ggplot(df, aes(X, Y)) + geom_line() + theme_minimal()
wage <- data(Wage)
head(Wage)
set.seed(1)
mse <- rep(NA, 10)
for (d in 1:10) {
poly_terms <- poly(Wage$age, d)
df <- cbind(wage = Wage$wage, as.data.frame(poly_terms))
fit <- train(wage ~ ., data = df, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
best_degree <- which.min(mse)
plot(1:10, mse, type = "b", xlab = "Degré", ylab = "MSE")
points(best_degree, mse[best_degree], col = "red", pch = 19)
# Ajustement
fit_final <- lm(wage ~ poly(age, best_degree), data = Wage)
ggplot(Wage, aes(age, wage)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", formula = y ~ poly(x, best_degree))
cv_err <- rep(NA, 10)
for (k in 2:10) {
Wage$age_cut <- cut(Wage$age, k)
fit <- train(wage ~ age_cut, data = Wage, method = "lm", trControl = trainControl(method = "cv", number = 10))
cv_err[k] <- mean(fit$resample$RMSE^2)
}
best_k <- which.min(cv_err)
Wage$age_cut <- cut(Wage$age, best_k)
fit_step <- lm(wage ~ age_cut, data = Wage)
plot(Wage$age, Wage$wage, pch = 19, cex = 0.5)
lines(Wage$age, predict(fit_step), col = "blue")
gam_fit <- gam(wage ~ s(age, 4) + maritl + jobclass, data = Wage)
plot(gam_fit, se = TRUE, col = "blue")
summary(gam_fit)
data(Auto)
plot(Auto$horsepower, Auto$mpg)
model_poly <- lm(mpg ~ poly(horsepower, 3), data = Auto)
lines(Auto$horsepower, predict(model_poly), col = "red")
summary(model_poly)
data(Boston)
head(grivr)
install.packages("ISLR2")
library(ISLR2)
data(Boston)
head(Boston)
# Cubic polynomial
poly_fit <- lm(nox ~ poly(dis, 3), data = Boston)
summary(poly_fit)
plot(Boston$dis, Boston$nox)
lines(sort(Boston$dis), predict(poly_fit, newdata = Boston[order(Boston$dis), ]), col = "red")
# Validation croisée
mse <- rep(NA, 10)
for (d in 1:10) {
fit <- train(nox ~ poly(dis, d), data = Boston, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
# Cubic polynomial
poly_fit <- lm(nox ~ poly(dis, 3), data = Boston)
summary(poly_fit)
plot(Boston$dis, Boston$nox)
lines(sort(Boston$dis), predict(poly_fit, newdata = Boston[order(Boston$dis), ]), col = "red")
# Validation croisée
for (d in 1:10) {
poly_terms <- poly(Boston$dis, d)
df <- cbind(Boston = Boston, as.data.frame(poly_terms))
fit <- train(wage ~ ., data = df, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
# Cubic polynomial
poly_fit <- lm(nox ~ poly(dis, 3), data = Boston)
summary(poly_fit)
plot(Boston$dis, Boston$nox)
lines(sort(Boston$dis), predict(poly_fit, newdata = Boston[order(Boston$dis), ]), col = "red")
# Validation croisée
for (d in 1:10) {
poly_terms2 <- poly(Boston$dis, d)
df <- cbind(Boston = Boston, as.data.frame(poly_terms2))
fit <- train(wage ~ ., data = df, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
# Cubic polynomial
poly_fit <- lm(nox ~ poly(dis, 3), data = Boston)
summary(poly_fit)
plot(Boston$dis, Boston$nox)
lines(sort(Boston$dis), predict(poly_fit, newdata = Boston[order(Boston$dis), ]), col = "red")
# Validation croisée
for (d in 1:10) {
poly_terms2 <- poly(Boston$dis, d)
df <- cbind(Boston = Boston, as.data.frame(poly_terms2))
fit <- train(Boston ~ ., data = df, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
# Cubic polynomial
poly_fit <- lm(nox ~ poly(dis, 3), data = Boston)
summary(poly_fit)
plot(Boston$dis, Boston$nox)
lines(sort(Boston$dis), predict(poly_fit, newdata = Boston[order(Boston$dis), ]), col = "red")
# Validation croisée
for (d in 1:10) {
poly_terms2 <- poly(Boston$dis, d)
df <- cbind(as.data.frame(poly_terms2), nox = Boston$nox)
fit <- train(nox ~ ., data = df, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
best_d <- which.min(mse)
# Spline
spline_fit <- lm(nox ~ bs(dis, df = 4), data = Boston)
plot(Boston$dis, Boston$nox)
lines(sort(Boston$dis), predict(spline_fit, newdata = Boston[order(Boston$dis), ]), col = "blue")
data(College)
set.seed(1)
train_index <- createDataPartition(College$Outstate, p = 0.7, list = FALSE)
train <- College[train_index, ]
test <- College[-train_index, ]
# Sélection par étapes
step_model <- step(lm(Outstate ~ ., data = train))
# GAM
library(gam)
gam_model <- gam(Outstate ~ s(Room.Board, 4) + s(PhD, 4) + Private, data = train)
par(mfrow = c(1, 3))
plot(gam_model, se = TRUE, col = "blue")
# Prédictions
pred <- predict(gam_model, newdata = test)
mean((pred - test$Outstate)^2)
summary(gam_fit)
View(College)
summary(gam_model)
data(College)
set.seed(1)
train_index <- createDataPartition(College$Outstate, p = 0.7, list = FALSE)
train <- College[train_index, ]
test <- College[-train_index, ]
# Sélection par étapes
step_model <- step(lm(Outstate ~ ., data = train))
summary(step_model)
library(gam)
gam_model <- gam(Outstate ~ s(Room.Board, 4) + s(PhD, 4) + Private + Apps + Accept + Top10perc + F.Undergrad +
Personal + Terminal + S.F.Ratio + perc.alumni + Expend + Grad.Rate, data = train)
par(mfrow = c(1, 3))
plot(gam_model, se = TRUE, col = "blue")
# Prédictions
pred <- predict(gam_model, newdata = test)
mean((pred - test$Outstate)^2)
summary(gam_model)
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(ISLR)
library(leaps)
library(splines)
library(gam)
library(caret)
f1 <- function(x, beta, xi) {
beta[1] + beta[2]*x + beta[3]*x^2 + beta[4]*x^3
}
f2 <- function(x, beta, xi) {
beta[1] + beta[2]*x + beta[3]*x^2 + beta[4]*x^3 + beta[5]*(x - xi)^3
}
# Continuité
d_continuity <- function(beta, xi) {
f1(xi, beta, xi) == f2(xi, beta, xi)
}
# Dérivée première
d1 <- function(x, beta, xi) {
beta[2] + 2*beta[3]*x + 3*beta[4]*x^2
}
d2 <- function(x, beta, xi) {
beta[2] + 2*beta[3]*x + 3*beta[4]*x^2 + 3*beta[5]*(x - xi)^2
}
# Dérivée seconde
d1_prime <- function(x, beta, xi) {
2*beta[3] + 6*beta[4]*x
}
d2_prime <- function(x, beta, xi) {
2*beta[3] + 6*beta[4]*x + 6*beta[5]*(x - xi)
}
plot(NA, xlim = c(0, 1), ylim = c(0, 1), xlab = "x", ylab = "g chapeau")
text(0.5, 0.9, "m=0 → constante")
text(0.5, 0.75, "m=1 → droite")
text(0.5, 0.6, "m=2 → polynôme degré 1")
text(0.5, 0.45, "m=3 → polynôme degré 2")
text(0.5, 0.3, "λ=0 → surajustement total")
X <- seq(-2, 2, length.out = 100)
Y <- 1 + 1*X + (-2)*(X - 1)^2 * (X >= 1)
df <- data.frame(X, Y)
ggplot(df, aes(X, Y)) + geom_line() + theme_minimal()
X <- seq(-2, 6, length.out = 100)
b1 <- (X >= 0 & X <= 2) - (X - 1)*(X >= 1 & X <= 2)
b2 <- (X - 3)*(X >= 3 & X <= 4) + (X > 4 & X <= 5)
Y <- 1 + 1*b1 + 3*b2
df <- data.frame(X, Y)
ggplot(df, aes(X, Y)) + geom_line() + theme_minimal()
wage <- data(Wage)
head(Wage)
set.seed(1)
mse <- rep(NA, 10)
for (d in 1:10) {
poly_terms <- poly(Wage$age, d)
df <- cbind(wage = Wage$wage, as.data.frame(poly_terms))
fit <- train(wage ~ ., data = df, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
best_degree <- which.min(mse)
plot(1:10, mse, type = "b", xlab = "Degré", ylab = "MSE")
points(best_degree, mse[best_degree], col = "red", pch = 19)
# Ajustement
fit_final <- lm(wage ~ poly(age, best_degree), data = Wage)
ggplot(Wage, aes(age, wage)) +
geom_point(alpha = 0.5) +
geom_smooth(method = "lm", formula = y ~ poly(x, best_degree))
cv_err <- rep(NA, 10)
for (k in 2:10) {
Wage$age_cut <- cut(Wage$age, k)
fit <- train(wage ~ age_cut, data = Wage, method = "lm", trControl = trainControl(method = "cv", number = 10))
cv_err[k] <- mean(fit$resample$RMSE^2)
}
best_k <- which.min(cv_err)
Wage$age_cut <- cut(Wage$age, best_k)
fit_step <- lm(wage ~ age_cut, data = Wage)
plot(Wage$age, Wage$wage, pch = 19, cex = 0.5)
lines(Wage$age, predict(fit_step), col = "blue")
gam_fit <- gam(wage ~ s(age, 4) + maritl + jobclass, data = Wage)
plot(gam_fit, se = TRUE, col = "blue")
summary(gam_fit)
data(Auto)
plot(Auto$horsepower, Auto$mpg)
model_poly <- lm(mpg ~ poly(horsepower, 3), data = Auto)
lines(Auto$horsepower, predict(model_poly), col = "red")
summary(model_poly)
library(ISLR2)
data(Boston)
head(Boston)
# Cubic polynomial
poly_fit <- lm(nox ~ poly(dis, 3), data = Boston)
summary(poly_fit)
plot(Boston$dis, Boston$nox)
lines(sort(Boston$dis), predict(poly_fit, newdata = Boston[order(Boston$dis), ]), col = "red")
# Validation croisée
for (d in 1:10) {
poly_terms2 <- poly(Boston$dis, d)
df <- cbind(as.data.frame(poly_terms2), nox = Boston$nox)
fit <- train(nox ~ ., data = df, method = "lm", trControl = trainControl(method = "cv", number = 10))
mse[d] <- mean(fit$resample$RMSE^2)
}
best_d <- which.min(mse)
# Spline
spline_fit <- lm(nox ~ bs(dis, df = 4), data = Boston)
plot(Boston$dis, Boston$nox)
lines(sort(Boston$dis), predict(spline_fit, newdata = Boston[order(Boston$dis), ]), col = "blue")
data(College)
set.seed(1)
train_index <- createDataPartition(College$Outstate, p = 0.7, list = FALSE)
train <- College[train_index, ]
test <- College[-train_index, ]
# Sélection par étapes
step_model <- step(lm(Outstate ~ ., data = train))
summary(step_model)
library(gam)
gam_model <- gam(Outstate ~ s(Room.Board, 4) + s(PhD, 4) + Private + Apps + Accept + Top10perc + F.Undergrad +
Personal + Terminal + S.F.Ratio + perc.alumni + Expend + Grad.Rate, data = train)
par(mfrow = c(1, 3))
plot(gam_model, se = TRUE, col = "blue")
# Prédictions
pred <- predict(gam_model, newdata = test)
mean((pred - test$Outstate)^2)
summary(gam_model)
shiny::runApp('UQAC/Courses/ETE 2025/RTC_Delmas')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas')
library(sodium)
print(sodium_version()) # Devrait afficher une version
remove.packages("sodium")
install.packages("sodium", type = "binary") # Essayer de forcer l'installation binaire si disponible
library(sodium)
print(sodium_version()) # Devrait afficher une version
install.packages("devtools")
library(sodium)
print(sodium_version()) # Devrait afficher une version
Sys.which("make")
## "C:\\rtools40\\usr\\bin\\make.exe"
install.packages("jsonlite", type = "source")
install.packages("jsonlite", type = "source")
writeLines('PATH="${RTOOLS43_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")
shiny::runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
help(package = "shinymanager")
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
install.packages("remotes")
remotes::install_github("datastorm-open/shinymanager")
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
runApp('UQAC/Courses/ETE 2025/RTC_Delmas/test2')
library(shinymanager)
read_db_decrypt("credentials.sqlite", passphrase = "THINKDIFFERENTTHINKBIG")
setwd("~/UQAC/Courses/ETE 2025/RTC_Delmas")
library(shinymanager)
read_db_decrypt("credentials.sqlite", passphrase = "THINKDIFFERENTTHINKBIG")
setwd("~/UQAC/Courses/ETE 2025/RTC_Delmas/test2")
library(shinymanager)
read_db_decrypt("credentials.sqlite", passphrase = "THINKDIFFERENTTHINKBIG")
runApp()
runApp()
runApp()
runApp()
# Script: create_credentials.R
library(shinymanager)
credentials_data <- data.frame(
user = c("dev@test.com", "pres@test.com"),
password = c("dev123", "pres123"),
role = c("Développeur", "Président"), # Assurez-vous que l'orthographe correspond exactement à roles_config
stringsAsFactors = FALSE
)
create_db(
credentials_data = credentials_data,
sqlite_path = "credentials.sqlite",
passphrase = "THINKDIFFERENTTHINKBIG"
)
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
git config --global commit.gpgSign false
runApp()
runApp()
runApp()
install.packages("RMariaDB")
runApp()
shiny::runApp()
runApp()
runApp()
shiny::runApp()
